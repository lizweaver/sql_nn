"""
Compare PyTorch predictions with Databricks SQL predictions.

This script loads the trained PyTorch model and compares its predictions
on the test set with the predictions generated by the SQL neural network
in Databricks.

Usage:
    python verify_predictions.py
    
Requirements:
    - DATABRICKS_HOST environment variable
    - DATABRICKS_HTTP_PATH environment variable
    - DATABRICKS_TOKEN environment variable
    - databricks-sql-connector: pip install databricks-sql-connector
"""

import torch
import pandas as pd
import numpy as np
import os
import sys

# Add initial_nn to path so we can import the model
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'initial_nn'))
from model import Net

try:
    from databricks import sql
    from torchvision import datasets, transforms
except ImportError as e:
    print(f"Error: Missing required package: {e}")
    print("\nInstall with:")
    print("  pip install databricks-sql-connector torchvision")
    sys.exit(1)


def load_pytorch_model(model_path):
    """Load the trained PyTorch model."""
    model = Net()
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.eval()
    return model


def get_pytorch_predictions(model, num_samples=10):
    """Get PyTorch predictions for the first N samples."""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    data_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'initial_nn', 'data')
    test_dataset = datasets.MNIST(data_dir, train=False, download=False, transform=transform)
    
    results = []
    for i in range(num_samples):
        image, label = test_dataset[i]
        with torch.no_grad():
            output = model(image.view(1, -1))
            logits = output.numpy()[0]
            pred = output.argmax(dim=1).item()
            confidence = logits[pred]
            
            results.append({
                'sample_id': i,
                'true_label': label,
                'pytorch_prediction': pred,
                'pytorch_logit': float(confidence)
            })
    
    return pd.DataFrame(results)


def get_databricks_predictions():
    """Fetch predictions from Databricks SQL warehouse."""
    # Check for required environment variables
    required_vars = ['DATABRICKS_HOST', 'DATABRICKS_HTTP_PATH', 'DATABRICKS_TOKEN']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"Error: Missing required environment variables: {', '.join(missing_vars)}")
        print("\nSet them with:")
        for var in missing_vars:
            print(f"  export {var}='your-value-here'")
        sys.exit(1)
    
    # Connect to Databricks
    connection = sql.connect(
        server_hostname=os.environ['DATABRICKS_HOST'],
        http_path=os.environ['DATABRICKS_HTTP_PATH'],
        access_token=os.environ['DATABRICKS_TOKEN']
    )
    
    cursor = connection.cursor()
    
    try:
        # Fetch predictions from validation table
        query = """
        SELECT 
            sample_id, 
            true_label,
            databricks_prediction, 
            databricks_logit,
            databricks_correct
        FROM workspace.neural_network.validation 
        ORDER BY sample_id
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        df = pd.DataFrame(results, columns=[
            'sample_id', 
            'true_label',
            'databricks_prediction', 
            'databricks_logit',
            'databricks_correct'
        ])
        
        return df
        
    finally:
        cursor.close()
        connection.close()


def compare_predictions(pytorch_df, databricks_df):
    """Compare PyTorch and Databricks predictions."""
    # Merge dataframes
    comparison = pytorch_df.merge(
        databricks_df, 
        on=['sample_id', 'true_label'],
        how='inner'
    )
    
    # Check if predictions match
    comparison['predictions_match'] = (
        comparison['pytorch_prediction'] == comparison['databricks_prediction']
    )
    
    # Check if logits are close (they might differ slightly due to floating point)
    comparison['logit_diff'] = abs(
        comparison['pytorch_logit'] - comparison['databricks_logit']
    )
    
    comparison['pytorch_correct'] = (
        comparison['pytorch_prediction'] == comparison['true_label']
    )
    
    return comparison


def print_results(comparison):
    """Print comparison results."""
    print("\n" + "="*80)
    print("PYTORCH vs DATABRICKS SQL NEURAL NETWORK COMPARISON")
    print("="*80)
    
    print("\nDetailed Comparison:")
    print("-" * 80)
    
    display_cols = [
        'sample_id', 
        'true_label', 
        'pytorch_prediction', 
        'databricks_prediction',
        'predictions_match',
        'logit_diff'
    ]
    
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    print(comparison[display_cols].to_string(index=False))
    
    print("\n" + "="*80)
    print("SUMMARY STATISTICS")
    print("="*80)
    
    total_samples = len(comparison)
    predictions_match = comparison['predictions_match'].sum()
    match_rate = (predictions_match / total_samples) * 100
    
    pytorch_accuracy = (comparison['pytorch_correct'].sum() / total_samples) * 100
    databricks_accuracy = (comparison['databricks_correct'].sum() / total_samples) * 100
    
    max_logit_diff = comparison['logit_diff'].max()
    avg_logit_diff = comparison['logit_diff'].mean()
    
    print(f"\nTotal samples compared: {total_samples}")
    print(f"Predictions match: {predictions_match}/{total_samples} ({match_rate:.1f}%)")
    print(f"\nPyTorch accuracy: {pytorch_accuracy:.1f}%")
    print(f"Databricks SQL accuracy: {databricks_accuracy:.1f}%")
    print(f"\nLogit differences:")
    print(f"  Average: {avg_logit_diff:.6f}")
    print(f"  Maximum: {max_logit_diff:.6f}")
    
    if match_rate == 100.0 and max_logit_diff < 0.01:
        print("\n✅ SUCCESS: Databricks SQL implementation matches PyTorch!")
    elif match_rate == 100.0:
        print("\n⚠️  PARTIAL SUCCESS: Predictions match but logits differ slightly")
        print("   (This is likely due to floating point precision differences)")
    else:
        print("\n❌ ERROR: Predictions do not match!")
        print("\nMismatches:")
        mismatches = comparison[~comparison['predictions_match']]
        print(mismatches[['sample_id', 'true_label', 'pytorch_prediction', 'databricks_prediction']])
    
    print("\n" + "="*80)


def main():
    """Main verification script."""
    print("\nStarting verification...")
    
    # Paths
    script_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(script_dir, '..', '..', 'initial_nn', 'model.pth')
    
    # Check if model exists
    if not os.path.exists(model_path):
        print(f"Error: Model file not found at {model_path}")
        sys.exit(1)
    
    print("\n1. Loading PyTorch model...")
    model = load_pytorch_model(model_path)
    
    print("2. Getting PyTorch predictions...")
    pytorch_df = get_pytorch_predictions(model, num_samples=10)
    
    print("3. Fetching Databricks SQL predictions...")
    try:
        databricks_df = get_databricks_predictions()
    except Exception as e:
        print(f"\nError fetching from Databricks: {e}")
        print("\nMake sure you have:")
        print("  1. Run 'dbt seed' to load the data")
        print("  2. Run 'dbt run --select tag:databricks' to generate predictions")
        print("  3. Set the required environment variables")
        sys.exit(1)
    
    print("4. Comparing predictions...")
    comparison = compare_predictions(pytorch_df, databricks_df)
    
    print_results(comparison)
    
    # Save detailed results
    output_path = os.path.join(script_dir, '..', 'verification_results.csv')
    comparison.to_csv(output_path, index=False)
    print(f"\nDetailed results saved to: {output_path}")


if __name__ == '__main__':
    main()

